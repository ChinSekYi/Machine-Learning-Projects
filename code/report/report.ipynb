{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Company Bankruptcy using Machine Learning\n",
    "By CS3244 Project Team 11  \n",
    "Cao Han  |  Chin Sek Yi  |  Lim Kai Sin  |  Luo Xinming  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "* [Chapter 1: Project Overview](#chapter1)\n",
    "    * [1.1 Project Motivation](#section1_1)\n",
    "    * [1.2 Dataset Description](#section1_2)\n",
    "    * [1.3 Methodologies](#section1_3)\n",
    "* [Chapter 2: Data Preparation](#chapter2)\n",
    "    * [2.1 Data Collection](#section2_1)\n",
    "    * [2.2 Data Exploration (EDA)](#section2_2)\n",
    "    * [2.3 Data Pre-Processing](#section2_3)\n",
    "* [Chapter 3: Modelling](#chapter3)\n",
    "    * [3.1 Evaluation Metrics](#section3_1)\n",
    "    * [3.2 Machine Learning Models](#section3_2)\n",
    "    * [3.3 Our Modelling Approach](#section3_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 1: Project Overview <a id=\"chapter1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Project Motivation <a id=\"section1_1\"></a>\n",
    "In today's dynamic business landscape, exemplified by recent events such as the bankruptcy of Silicon Valley Bank, the ability to anticipate and mitigate financial risks is crucial for sustainable growth and stability. This project aims to develop a robust predictive model of company bankruptcy, leveraging advanced machine learning algorithms and financial data analysis techniques, so as to equip stakeholders with nuanced insights to confidently traverse the unpredictable landscape of financial risk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Dataset Description <a class=\"anchor\" id=\"section1_2\"></a>\n",
    "The dataset used in this project is about bankruptcy prediction of Polish companies. The dataset contains financial rates from one year and corresponding class label that indicates bankruptcy status 3 years after that year. The data contains 10503 instances (financial statements), with 495 representing bankrupted companies and 10008 still operating at the end of the 3-year forecasting period.\n",
    "\n",
    "The data was collected from [Emerging Markets Information Service](http://www.securities.com), which is a database containing information on emerging markets around the world. The bankrupt companies were analyzed in the period 2000-2012, while the still operating companies were evaluated from 2007 to 2013.  \n",
    "\n",
    "Source: [UC Irvine Machine Learning Repository](https://archive.ics.uci.edu/dataset/365/polish+companies+bankruptcy+data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Methodologies <a class=\"anchor\" id=\"section1_3\"></a>\n",
    "This project is structured into five distinct steps:\n",
    "\n",
    "1. **Data Collection**: This initial phase involves setting up a web scraper to automate the acquisition and transformation of the desired dataset. The process is designed to systematically download and convert data, streamlining subsequent analysis.\n",
    "\n",
    "2. **Exploratory Data Analysis (EDA)**: In this step, various visualization tools such as histograms and heatmaps are employed to explore the data distribution and examine the relationships between features. This analysis helps identify patterns and insights that inform further data handling strategies.\n",
    "\n",
    "3. **Data Pre-processing**: During this stage, missing values are addressed through mean imputation, ensuring no data is unnecessarily discarded. Additionally, numerical values are standardized to neutralize disparities in scale among features, enhancing model accuracy. To mitigate the effects of multicollinearity, features that show high correlation with others are selectively removed. As the dataset is highly imbalanced, we used Synthetic Minority Over-sampling Technique (SMOTE) to generate instances of the minortity class for training. \n",
    "\n",
    "4. **Modeling**: Various well-established machine learning algorithms are utilized in this step, including Support Vector Machines (SVM), K-Nearest Neighbors (KNN), and Decision Trees, to develop predictive models for predicting company bankruptcy. Logistic regression serves as a baseline for comparison. Advanced techniques such as bagging and boosting are implemented to improve each model's robustness. Furthermore, the potential of neural networks is explored to assess their efficacy in enhancing predictive performance.\n",
    "\n",
    "5. **Evaluation and Comparison**: The final step involves a thorough evaluation and comparison of each model's performance. The outcomes are meticulously analyzed to identify the most effective model, which is then detailed in this section, highlighting the relative advantages and effectiveness of the approaches used.\n",
    "\n",
    "This structured approach ensures a comprehensive analysis and robust development of predictive models, aiming to deliver reliable and actionable insights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 2: Data Preparation <a class=\"anchor\" id=\"chapter2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Data Collection <a class=\"anchor\" id=\"section2_1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import zipfile\n",
    "import os\n",
    "from scipy.io import arff\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_url = \"https://archive.ics.uci.edu/static/public/365/polish+companies+bankruptcy+data.zip\"\n",
    "response = requests.get(download_url)\n",
    "zip_file_path = \"downloaded_file.zip\"\n",
    "\n",
    "with open(zip_file_path, \"wb\") as file:\n",
    "    file.write(response.content)\n",
    "\n",
    "def extract_arff_from_zip(zip_path, arff_filename, extraction_path='.'):\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extract(arff_filename, path=extraction_path)\n",
    "        return os.path.join(extraction_path, arff_filename)\n",
    "\n",
    "def convert_arff_to_csv(arff_path, csv_path):\n",
    "    data, meta = arff.loadarff(arff_path)\n",
    "    df = pd.DataFrame(data)    \n",
    "    for col in df.select_dtypes([object]):\n",
    "        if isinstance(df[col][0], bytes):\n",
    "            df[col] = df[col].apply(lambda x: x.decode('utf-8'))\n",
    "    df.to_csv(csv_path, index=False)\n",
    "\n",
    "for i in range(1, 6):\n",
    "    arff_filename = f'{i}year.arff'\n",
    "    csv_path = f'{i}year.csv'\n",
    "    # extracted_arff_path = extract_arff_from_zip(zip_file_path, arff_filename)\n",
    "    # convert_arff_to_csv(extracted_arff_path, csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Data Exploration (EDA) <a class=\"anchor\" id=\"section2_2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Data Pre-Processing <a class=\"anchor\" id=\"section2_3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 3: Modelling <a class=\"anchor\" id=\"chapter3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Evaluation Metrics  <a class=\"anchor\" id=\"section3_1\"></a>\n",
    "\n",
    "Our primary task is to predict bankruptcy (a classification problem). Below are the key classification metrics that we will use to evaluate our model performance:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.1 Accuracy\n",
    "Accuracy provides a ratio of correctly predicted observations to the total observations. It is especially useful when the classes are balanced with SMOTE.\n",
    "\n",
    "**Formula**:\n",
    "$$\\text{Accuracy} = \\frac{\\text{Number of Correct Predictions}}{\\text{Total Number of Predictions}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.2 Confusion Matrix and Related Terms\n",
    "The confusion matrix is a table layout that allows visualization of the performance of the algorithm, where each number in the matrix represents:\n",
    "- **TP (True Positives)**: Correctly predicted positive observations.\n",
    "- **TN (True Negatives)**: Correctly predicted negative observations.\n",
    "- **FP (False Positives)**: Incorrectly predicted as positive.\n",
    "- **FN (False Negatives)**: Incorrectly predicted as negative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.3 Precision, Recall, and F1-Score\n",
    "These metrics offer a deeper understanding of the model's performance by taking into account data imbalances, thereby providing a more nuanced view of the accuracy across different class labels.\n",
    "\n",
    "- **Precision**:\n",
    "$$ \\text{Precision} = \\frac{\\text{TP}}{\\text{TP} + \\text{FP}} $$\n",
    "Precision measures the accuracy of positive predictions.\n",
    "\n",
    "- **Recall** (or Sensitivity or TPR):\n",
    "$$ \\text{Recall} = \\frac{\\text{TP}}{\\text{TP} + \\text{FN}} $$\n",
    "Recall measures the ability of a model to find all the relevant cases (all positive samples).\n",
    "\n",
    "- **F1-Score**:\n",
    "$$ \\text{F1-Score} = 2 \\times \\left( \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}} \\right) $$\n",
    "The F1-Score is the weighted average of Precision and Recall. Therefore, this score takes both false positives and false negatives into account. A high F1-score shows a model can classify the positive class correctly, while not misclassifying many negative classes as positive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.4. Macro Average vs Micro Average\n",
    "\n",
    "- **Macro Average**: Macro average calculates the metric independently for each class and then takes the average (hence treating all classes equally). It is useful equal weight is given to the performance of each class, regardless of its frequency.\n",
    "\n",
    "- **Micro Average**: Micro average aggregates the contributions of all classes to compute the average metric. In other words, Micro Average will compute the overall total TP, FP, and FN across all classes, and then use these totals to calculate the performance scores. It is useful when metrics are weighted by class size, which is ideal for imbalanced data as it reflects the contribution of each class proportionally to its size.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import evaluation metrics\n",
    "from sklearn.metrics import (accuracy_score, classification_report,\n",
    "                             confusion_matrix, f1_score, precision_score,\n",
    "                             recall_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Machine Learning Models "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scikit-learn (sklearn)\n",
    "- **KNeighborsClassifier**: This model implements the k-nearest neighbors voting algorithm.\n",
    "- **LogisticRegression**: A model that applies logistic regression for binary classification tasks.\n",
    "- **SVC**: Support Vector Machine classifier known for its effectiveness in high-dimensional spaces.\n",
    "- **DecisionTreeClassifier**: A model that uses a decision tree for classification, useful for interpretability.\n",
    "- **GradientBoostingClassifier**: An ensemble model that builds on weak prediction models to create a strong classifier.\n",
    "- **RandomForestClassifier**: A meta estimator that fits a number of decision tree classifiers to various sub-samples of the dataset and uses averaging to improve predictive accuracy and control overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imbalanced-learn (imblearn)\n",
    "Dealing with imbalanced data:\n",
    "\n",
    "- **BalancedRandomForestClassifier**: A variation of the RandomForest that handles imbalances by adjusting weights inversely proportional to class frequencies in the input data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost (xgboost)\n",
    "\n",
    "- **XGBoost**: An implementation of gradient boosted decision trees designed for speed and performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "\n",
    "# imblearn\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "\n",
    "# XGBoost\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Our Modelling Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.1 Simple logistic regression\n",
    "We begin our modeling with a simple logistic regression to establish a baseline for benchmark performance.\n",
    "This approach provides an initial overview of how well simple models can predict outcomes based on our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Classification Report:\\n\", class_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.2 K-Nearest Neighbors (KNN)\n",
    "We continue our modeling with the K-Nearest Neighbors (KNN) algorithm to compare its performance against the baseline established by logistic regression. KNN is a technique that predicts the class of a given point based on the majority vote of its nearest neighbors. KNN is useful for gaining insights into the datasetâ€™s structure due to its reliance on feature similarity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# knn code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the high dimensionality of our dataset, even after data cleaning, the KNN model might not perform optimally due to its known limitations with high-dimensional data. To address this issue, we apply Principal Component Analysis (PCA) to reduce the dimensionality. This step aims to enhance the performance of our KNN model by focusing on the most significant features and reducing the noise associated with less important variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# knn with pca code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.3 Support Vector Machine (SVM)\n",
    "We further our analysis by incorporating a Support Vector Machine (SVM) model to assess its effectiveness compared to the simpler models previously tested. SVM is a classification technique that finds the optimal hyperplane which best separates the data into different classes. It addresses the short comings of KNN, as it performs well in high-dimensional space and work well with non-linearly separable data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# SVM code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Support Vector Machine (SVM) model has not performed as expected in our analysis for several reasons:\n",
    "\n",
    "1. **Sensitivity to Outliers**: SVM is particularly sensitive to outliers. In our dataset, we retained outliers to capture distinctive characteristics of different companies with respect to their bankruptcy status. This sensitivity can lead to skewed decision boundaries, adversely affecting model performance. \n",
    "\n",
    "2. **Overfitting Issues**: In this analysis, the SVM model has shown a tendency to overfit. This issue stems from the lack of an appropriate regularization function. Without proper regularization, SVM models can overly conform to the noise in the training data rather than capturing the general pattern, leading to poor generalization on new, unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 5: Result Evaluation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
